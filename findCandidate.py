import jieba
from bert4keras.snippets import to_array
from bert4keras.tokenizers import Tokenizer
from bert4keras.models import build_transformer_model
from bert4keras.backend import keras
import time
import torch.nn.functional as F
import torch
import tensorflow as tf
import os
import numpy as np
import pyprind
import pickle
import os
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"


class CosineSimilarityTest(torch.nn.Module):
    """
    matrix cosine similarity compute
    """

    def __init__(self):
        super(CosineSimilarityTest, self).__init__()

    def forward(self, x1, x2):
        x2 = x2.t()
        x = x1.mm(x2)

        x1_frobenius = x1.norm(dim=1).unsqueeze(0).t()
        x2_frobenins = x2.norm(dim=0).unsqueeze(0)
        x_frobenins = x1_frobenius.mm(x2_frobenins)

        final = x.mul(1 / x_frobenins)
        return final


def getRow(pos):
    """
    get word's row index from corpus
    """
    return pos[0]


def getColumn(pos):
    """
    get word's column index from corpus
    """
    return pos[1]


if __name__ == '__main__':

    # load your pretrained model
    config_path = './DC-BERT/bert_config.json'
    checkpoint_path = './DC-BERT/bert_model.ckpt'
    dict_path = './DC-BERT/vocab.txt'

    tokenizer = Tokenizer(
        dict_path, pre_tokenize=lambda s: jieba.cut(s))  # load tokenizer
    model = build_transformer_model(
        config_path, checkpoint_path)  # build DC-BERT model

    # read from segmented corpus
    corpus = [line.split() for line in open(
        "./cut.csv", 'r', encoding='utf-8').readlines()]

    # read from contextual embeddings
    with open("./bert_final_emb.pickle", "rb") as f:
        emb = pickle.load(f)

    # a list of seed criminal keywords for readers to reference, you can either delete or add words related to your task
    with open("./keywords.json", "r") as f:
        Keyword = json.load(f)

    # read from relevant scores list generated by findSeedKeywords.py
    words = [line.split() for line in open(
        "./scores/dp_score.txt", 'r', encoding='utf-8').readlines()]

    # using drug as an example
    KeyW = Keyword["毒品"]
    KeyW.insert(0, '[CLS]')
    KeyW.insert(len(KeyW), '[SEP]')
    # print(KeyW)

    # generate seed keyword embeddings
    token_ids, segment_ids = tokenizer.encode(KeyW, flag=True)
    KeyV = model.predict([np.array([token_ids]), np.array([segment_ids])])
    # print(type(KeyV))
    # print(KeyV.shape)

    pbar = pyprind.ProgBar(len(words), title='进度展示', monitor=True)
    group_vector_output = np.array([]).reshape(0, 768)

    t1 = time.time()

    # traverse the words and concat their embeddings
    for word in words:
        sen = corpus[int(word[2])]
        senten = emb[int(word[2])]
        # find the corresponding embedding according to position
        vec = senten[sen.index(word[0]) + 1]
        group_vector_output = np.vstack([group_vector_output, vec])
        pbar.update()
    # print(group_vector_output.shape)

    # load embeddings to device
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    x1 = torch.from_numpy(group_vector_output).float().to(device)
    x2 = torch.from_numpy(KeyV[0]).float().to(device)

    # the process of loading embeddings is more time-consuming
    t2 = time.time()
    print("加载时间为：{}".format(t2 - t1))

    start_time = time.time()
    simmodel = CosineSimilarityTest().to(device)

    # 同时需要多卡计算时需要
    # model = torch.nn.DataParallel(model)

    final_value = simmodel(x1, x2)
    # print(final_value.size())
    # average the similarity between seed keywords and words
    sims = torch.mean(final_value, 1)
    # 输出排序并输出topk的输出
    #value, indec = torch.topk(final_value, 3, dim=0, largest=True, sorted=True)
    # print(value)

    end_time = time.time()
    print("计算时间为：{}".format(end_time - start_time))

    # save the drug jargon candidate to DPCANDIDATE.txt
    with open("./CANDIDATE/DPCANDIDATE.txt", "w") as f:
        for word, sim in zip(words, sims):
            f.write(word[0] + " ")  # the word
            f.write(str(sim.item()))  # and its similarity
            f.write("\n")
